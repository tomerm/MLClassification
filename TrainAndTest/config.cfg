[root]
# Root folder of the data. Empty value means user's home.
home =
# Show plots
showPlots = no

[preprocess]
# Defines the need in language-specific tokenization.
actualToks = yes
# Relative path to the folder, containing source data. Example: data/bbcnews/source.
sourcePath = MLClassificationData/bbcnews
# Relative path to the folder, containing results of tokenization. Example: data/bbcnews/target.
targetPath = MLClassificationData/bbcnews3
# Relative path to the tagger's jar
taggerPath = MLClassification/tokenizer/arabic/stanford/taggers/PyArabicTokenizer.jar
# Relative path to runtime version of tagger. The same jar used by DataLoader.
rtTaggerPath = MLClassification/tokenizer/arabic/stanford/taggers/ArabicDocumentsTokenizer.jar
# List of POS's, which should be excluded from results of tokenization.
exPOS = PUNC,DT,IN,CD,PRP,RP,RB,W,PDT
# Need in text normalization.
normalization = yes
# Need to exclude stop words from results of tokenization.
stopWords = yes
# List of extra words, which should be excluded from results of tokenization.
extraWords =

[word_embedding]
# Need to recreate W2V model.
w2vCreate = yes
# Relative path to the text corpus.
w2vCorpusPath = MLClassificationData/w2v/target/wiki_ar.txt
# Dimentions of vectors.
w2vDim = 100
# Count epochs in training
w2vEpochs = 100
# Add time stamp to the model's name
w2vTimeInName = no
# Relative path to W2V vectors file
w2vModelPath = MLClassificationData/w2v/vectors/W2VModel.vec

[data]
# Need in language-specific tokenization
actualToks = yes
# Relative path to the folder, containing train or all data.
#trainPath = MLClassificationData/train/rtanews/target
trainPath = MLClassificationData/train/rtanews/target
# Relative path to the folder, containing test data.
testPath = MLClassificationData/test/rtanews/target
# Size of test data set as a part of train data set (used if testPath is empty).
testSize = 0
# Size of validation data set as a part of train data set.
valSize = 0.15
# List of categories, excluded from training and testing.
exCats =
# Need to show data set analysis.
analysis = no
# Need to load w2v model
w2vLoad = yes
# Tokenization of loaded data
dataToks = yes
# Path to the folder, containing actual documents for testing
actualPath = MLClassificationData/test/rtanews/source

[model]
#  Type of the model. One of SNN, LTSM, CNN, PAC, Perceptron, Ridge, SGD, SVC, BERT
type =
#  Name of the model.
name =
# Default count of epochs in training.
epochs = 20
# Batch size for training.
trainBatch = 128
# Batch size for testing.
testBatch = 8
# Training and testing verbose.
verbose = 1
# Need to save intermediate results.
tempSave = yes
# Relative path to the folder with intermediate results.
tempPath = MLClassificationData/models/temp
# Relative path to created model.
modelPath = MLClassificationData/models
# Path to indexer
indexerPath = MLClassificationData/indexers/indexer.pkl
# Path to binarizer
binarizerPath = MLClassificationData/indexers/mlb.pkl
# Path to vectorizer
vectorizerPath = MLClassificationData/indexers/wev.pkl
# Pre-trained BERT model path
bertPath = MLClassificationData/pybert/pytorch_bert.gz
# Path to folder with resulting BERT files
bertOutPath = MLClassificationData/pybert/out
# Type of execution. One of trainAndTest, train, test, crossValidation and none
runFor = trainandtest
# Count of cross-validation's loops.
kfold = 10
# Need to save datasets, correpond to cross-val. cycle with the best results
cvSave = yes
# Path to the folder containing train and test datasets used in cross-val. loop with the best results
cvPath = MLClassificationData/crossValidation
# Show results of testing
showMetrics = yes

[collector]
# Show consolidated results
showResults = no
# Calculate and save reports
reports = yes
# Path to the folder containing reportsccccccccccccccc
reportsPath = MLClassificationData/reports
# Prepare resources for runtime
saveResources = yes
# Path to the folder containing saved resources
resourcesPath = MLClassificationData/runtime

# === Requests ===
# Request defines the pipe - chain of processes, in which previous processes can prepare data or set parameters
# for subsequent. Processes are separated by symbol '|'. Currently we support 5 types of processes:
# W - word embedding
# P - preprocess
# D - data loading
# M - create/train/test model
# C - collector (consolidate results of model's testing and save resources for runtime)
# Each process has the following structure:
# <Symbol_of_process>(<list_of_parameters)
# List of parameters is a list of configuration's options, which should be changed for current and subsequent
# processes.
[requests]
request = D(w2vload=no) | M(type=svc; name=svc; runfor=trainAndTest) | C()
#request = P()
#request = D(datatoks=yes) | M(type=snn; name=snn; epochs=30) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(w2vload=no) | M(type=cnn; name=cnn; epochs=10) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(w2vload=no) | M(type=perceptron; name=perceptron; runfor=crossvalidation)
#request = D(w2vload=no; trainPath=MLClassificationData/crossValidation/train; testPath=MLClassificationData/crossValidation/test) | M(type=perceptron; name=perceptron)
infofrom = 2 days
# days(s), today
