[root]
# [mandatory] Root folder of the data. Empty value means user's home.
home = /home/user/PROJECTS
# [optinal, default: False] Show plots
show_plots = False

[preprocess]
# [optinal, default: True] Set False for pure Python version of Stanford NLP
use_java = True
# [optinal, default: True] Defines the need in language-specific tokenization.
language_tokenization = True
# [mandatory] Relative path to the folder, containing source data. Example: data/bbcnews/source.
source_path = MLClassificationData/bbcnews
# [mandatory] Relative path to the folder, containing results of tokenization. Example: data/bbcnews/target.
target_path = MLClassificationData/bbcnews3
# [mandatory] Relative path to the tagger's jar (wrapper over Stanford Java NLP library, including stopwords etc.)
set_of_docs_lang_tokenization_lib_path = MLClassification/tokenizer/arabic/stanford/taggers/PyArabicTokenizer.jar
# [mandatory] Relative path to runtime version of tagger. The same jar used by DataLoader.
# It is wrapper over Stanford Java NLP library only. Preprocessing including stopwords etc. is done in Python code
single_doc_lang_tokenization_lib_path = MLClassification/tokenizer/arabic/stanford/taggers/ArabicDocumentsTokenizer.jar
# [optinal, default: ''] List of POS's, which should be excluded from results of tokenization.
exclude_positions = PUNC,DT,IN,CD,PRP,RP,RB,W,PDT
# [optinal, default: True] if text normalization needed.
normalization = True
# [optinal, default: True] Need to exclude stop words from results of tokenization.
stop_words = True
# [optinal, default: ''] List of extra words, which should be excluded from results of tokenization.
extra_words =

[word_embedding]
# [optinal, default: True] if recreate W2V model needed.
need_create_model = True
# [mandatory] Relative path to the text corpus.
data_corpus_path = MLClassificationData/w2v/target/wiki_ar.txt
# [optinal, default: 100] Dimentions of vectors.
vectors_dimension = 100
# [optinal, default: 100] Count epochs in training
epochs_total = 100
# [optinal, default: False] Add time stamp to the model's name
include_current_time_in_model_name = False
# [mandatory] Relative path to W2V vectors file
model_path = MLClassificationData/w2v/vectors/W2VModel.vec

[data]
# [optinal, default: True] If language-specific tokenization needed
language_tokenization = True
# [mandatory] Relative path to the folder, containing train or all data.
train_data_path = MLClassificationData/train/rtanews/target
# [mandatory] Relative path to the folder, containing test data.
test_data_path = MLClassificationData/test/rtanews/target
# [optinal, default: 0] Size of test data set as a part of train data set (used if test_data_path is empty).
test_data_size = 0
# [optinal, default: 0.15] Size of validation data set as a part of train data set.
validation_data_size = 0.15
# [optinal, default: ''] List of categories, excluded from training and testing.
exclude_categories =
# [optinal, default: False] If show data set analysis needed.
analysis = False
# [optinal, default: True] If load w2v model neded
load_w2v_model = True
# [optinal, default: False] Tokenization of loaded data
enable_tokenization = False
# [mandatory] Path to the folder, containing actual documents for testing
actual_path = MLClassificationData/test/rtanews/source

[model]
#  [optinal, default: ''] Type of the model. One of snn, ltsm, cnn, pac, perceptron, ridge, sgd, svc, bert
type = snn
#  [optinal, default: ''] Name of the model.
name =
# [optinal, default: 20] Default count of epochs in training.
epochs = 20
# [optinal, default: 128] Batch size for training.
train_batch = 128
# [optinal, default: 8] Batch size for testing (not implemented?).
test_batch = 8
# [optinal, default: 1] Training and testing verbose.
verbose = 1
# [optinal, default: True] Need to save intermediate results.
save_intermediate_results = True
# [mandatory] Relative path to the folder with intermediate results.
intermediate_results_path = MLClassificationData/models/temp
# [mandatory] Relative path to created model.
created_model_path = MLClassificationData/models
# [mandatory] Path to indexer
indexer_path = MLClassificationData/indexers/indexer.pkl
# [mandatory] Path to binarizer
binarizer_path = MLClassificationData/indexers/mlb.pkl
# [mandatory] Path to vectorizer
vectorizer_path = MLClassificationData/indexers/wev.pkl
# [mandatory] Pre-trained BERT model path
pretrained_bert_model_path = MLClassificationData/pybert/pytorch_bert.gz
# [mandatory] Path to folder with resulting BERT files
resulting_bert_files_path = MLClassificationData/pybert/out
# [optinal, default: trainandtest] Type of execution. One of trainandtest, train, test, crossValidation and none
type_of_execution = trainandtest
# [optinal, default: 2] Amount of cross-validation's loops.
cross_validations_total = 2
# [optinal, default: True] Need to save datasets, correpond to cross-validation. loop with the best results
save_cross_validations_datasets = True
# [mandatory] Path to the folder containing train and test datasets used in cross-validation. loop with the best results
cross_validations_datasets_path = MLClassificationData/crossValidation
# [optinal, default: True] Show results of testing
show_test_results = True
# [optinal, default: True] Custom rank threshold
customrank = True
# [optinal, default: 0.5] minimum probability to predict specific label
rank_threshold = 0.5

[collector]
# [optinal, default: False] Show consolidated results
show_consolidated_results = False
# [optinal, default: True] Calculate and save reports
save_reports = True
# [mandatory] Path to the folder containing reports
reports_path = MLClassificationData/reports
# [optinal, default: True] Prepare resources for runtime
prepare_resources_for_runtime = True
# [mandatory] Path to the folder containing saved resources
saved_resources_path = MLClassificationData/runtime
# [optinal, default: True] Custom consolidated rank threshold
consolidatedrank = True
# [optinal, default: 0.5]  Custom rank threshold (part of models in chain) for consolidated results
consolidated_rank_threshold = 0.5

# === Requests ===
# Request defines the pipe - chain of processes, in which previous processes can prepare data or set parameters
# for subsequent. Processes are separated by symbol '|'. Currently we support 5 types of processes:
# W - word embedding
# P - preprocess
# D - data loading
# M - create/train/test model
# C - collector (consolidate results of model's testing and save resources for runtime)
# Each process has the following structure:
# <Symbol_of_process>(<list_of_parameters)
# List of parameters is a list of configuration's options, which should be changed for current and subsequent
# processes.

[requests]
request=D(load_w2v_model=True;enable_tokenization=True)|M(type=svc;name=svc;type_of_execution=crossvalidation)|C()
info_from = 2 days

# Use single doc tokenization
#request = D(load_w2v_model=False; enable_tokenization=True) | M(type=svc; name=svc; type_of_execution=crossvalidation) | C(saveResources=True)

# Use set of docs tokenization
#request = D(load_w2v_model=False; enable_tokenization=False) | P() | M(type=svc; name=svc; type_of_execution=crossvalidation)

# No tokenization
#request = D(load_w2v_model=False) | M(type=svc; name=svc; type_of_execution=crossvalidation)

#request = P()
#request = D(enable_tokenization=True) | M(type=snn; name=snn; epochs=30) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(load_w2v_model=False) | M(type=cnn; name=cnn; epochs=10) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(load_w2v_model=False) | M(type=perceptron; name=perceptron; type_of_execution=crossvalidation)
#request = D(load_w2v_model=False; train_data_path=MLClassificationData/crossValidation/train; test_data_path=MLClassificationData/crossValidation/test) | M(type=perceptron; name=perceptron)

